{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923d2a01",
   "metadata": {},
   "source": [
    "# City Shadows: Scene Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d8b8a",
   "metadata": {},
   "source": [
    "This notebook analyzes Google Street View images to identify individual buildings and trees, and records their measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f69d2-9079-4d9b-8c05-7ce2077a63b2",
   "metadata": {},
   "source": [
    "## I. PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890395e-baaf-4f5f-9a9e-5058a492eb5f",
   "metadata": {},
   "source": [
    "### ✅ 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Instance/Panoptic segmentation\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Depth estimation\n",
    "from transformers import DPTForDepthEstimation, DPTFeatureExtractor\n",
    "\n",
    "# new\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.ndimage import grey_dilation\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896a10e-e0e4-4c0f-8805-d5f13efd8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CUDA instead of CPU\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Device Count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705f80e-7ed6-4e97-b5df-a4d12e19fdf6",
   "metadata": {},
   "source": [
    "### ✅ 2. Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_ungrouped_path = \"../data/input/input_makati_ungrouped\" # ungrouped images\n",
    "input_grouped_path = \"../data/input/input_makati_grouped\" # grouped images by coordinates\n",
    "input_vanishing_path = \"../data/input/input_vanishing_reference\"\n",
    "\n",
    "output_depth_path = \"../data/output/output_depth\"\n",
    "output_semantic_path = \"../data/output/output_semantic\"\n",
    "output_instance_path = \"../data/output/output_instance\"\n",
    "output_panoptic_path = \"../data/output/output_panoptic\"\n",
    "output_json_path = \"../data/output/output_json\"\n",
    "output_final_json_path = \"../data/output/output_final_json\"\n",
    "\n",
    "save_depth_path = \"../data/save/save_depth\"\n",
    "save_panoptic_path = \"../data/save/save_panoptic\"\n",
    "save_semantic_path = \"../data/save/save_semantic\"\n",
    "save_instance_path = \"../data/save/save_instance\"\n",
    "\n",
    "color_map_path = \"../data/color_map/color_map_modified.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2c588-ab53-41cd-823b-931427941e16",
   "metadata": {},
   "source": [
    "### ✅ 3. Group GSV Images by Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733e999-8230-4c81-9105-bbfbb74615b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group images into folders by their coordinates\n",
    "image_list = [f for f in os.listdir(input_ungrouped_path) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "total_images = len(image_list)\n",
    "\n",
    "for i, image_name in enumerate(tqdm(image_list, desc=\"Processing images\")):\n",
    "    image_path = os.path.join(input_ungrouped_path, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract folder name (string before the underscore)\n",
    "    folder_name = image_name.split(\"_\")[0]\n",
    "    folder_path = os.path.join(input_grouped_path, folder_name)\n",
    "\n",
    "    # Create the folder if it doesn’t exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Save a copy of the image inside a folder of the input_grouped_path\n",
    "    output_path = os.path.join(folder_path, image_name)\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "    # Delete the original image from the input_ungrouped_path\n",
    "    os.remove(image_path) #optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b84d7-9408-47ba-8a5c-fd5b6876778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List folders with less than 8 images\n",
    "parent_dir = input_grouped_path\n",
    "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "folders_with_few_images = []\n",
    "\n",
    "for folder_name in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(image_extensions)]\n",
    "        if len(image_files) < 8:\n",
    "            folders_with_few_images.append((folder_name, len(image_files)))\n",
    "\n",
    "for folder, count in folders_with_few_images:\n",
    "    print(f\"{folder}: {count} image(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cc1c4-6d19-49e8-9d74-64ce8c0f3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# For testing purposes only\n",
    "def rename_files_with_prefix(root_folder, prefix=\"semantic_\"):\n",
    "    # Traverse all subfolders\n",
    "    for subdir, _, files in tqdm(os.walk(root_folder), desc=\"Renaming files\"):\n",
    "        for filename in files:\n",
    "            # Skip if already starts with prefix\n",
    "            if filename.startswith(prefix):\n",
    "                continue\n",
    "\n",
    "            # Construct full paths\n",
    "            old_path = os.path.join(subdir, filename)\n",
    "            new_filename = prefix + filename\n",
    "            new_path = os.path.join(subdir, new_filename)\n",
    "\n",
    "            # Rename file\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "# Example usage\n",
    "root_folder = output_semantic_path\n",
    "rename_files_with_prefix(root_folder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a9d18-ae29-425d-a51b-99d5e1cc4894",
   "metadata": {},
   "source": [
    "## II. PANOPTIC-SEMANTIC SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcba9d0-8081-4163-8683-8d8f4c77c4d2",
   "metadata": {},
   "source": [
    "### ✅ 1. Save Panoptic Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384888e-fb46-40fe-bd44-300984ba6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Detectron2 configuration and weights\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\")\n",
    "\n",
    "# Save the config\n",
    "os.makedirs(save_panoptic_path, exist_ok=True)\n",
    "with open(f\"{save_panoptic_path}/config.yaml\", \"w\") as f:\n",
    "    f.write(cfg.dump())\n",
    "\n",
    "# Save model weights\n",
    "torch.save(cfg.MODEL.WEIGHTS, f\"{save_panoptic_path}/model.pth\")\n",
    "print(f\"Model config and weights saved to {save_panoptic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d3295-d3aa-4dfa-989e-63d493704d44",
   "metadata": {},
   "source": [
    "### ✅ 2. Load Panoptic Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77703d-2d41-4896-857f-02947091ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Detectron2 model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(f\"{save_panoptic_path}/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = torch.load(f\"{save_panoptic_path}/model.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # confidence threshold\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "panoptic_model = DefaultPredictor(cfg)\n",
    "\n",
    "print(\"Detectron2 loaded successfully!\")\n",
    "print(\"Detectron2 is using:\", cfg.MODEL.DEVICE)\n",
    "\n",
    "# Use the dataset's labels\n",
    "if cfg.DATASETS.TRAIN:\n",
    "    dataset_name = cfg.DATASETS.TRAIN[0]\n",
    "    coco_metadata = MetadataCatalog.get(dataset_name)\n",
    "    print(\"Dataset name:\", dataset_name)\n",
    "    \n",
    "    print(\"Number of thing classes:\", len(coco_metadata.thing_classes))\n",
    "    print(\"Number of stuff classes:\", len(coco_metadata.stuff_classes))\n",
    "    \n",
    "    # Print thing classes with contiguous IDs\n",
    "    if hasattr(coco_metadata, \"thing_classes\"):\n",
    "        print(\"\\nThing Classes:\")\n",
    "        for contiguous_id, class_name in enumerate(coco_metadata.thing_classes):\n",
    "            color = coco_metadata.thing_colors[contiguous_id] if hasattr(coco_metadata, \"thing_colors\") else \"N/A\"\n",
    "            print(f\"- ID {contiguous_id}: {class_name}, Color: {color}\")\n",
    "    \n",
    "    # Print stuff classes with contiguous IDs\n",
    "    if hasattr(coco_metadata, \"stuff_classes\"):\n",
    "        print(\"\\nStuff Classes:\")\n",
    "        for contiguous_id, class_name in enumerate(coco_metadata.stuff_classes):\n",
    "            color = coco_metadata.stuff_colors[contiguous_id] if hasattr(coco_metadata, \"stuff_colors\") else \"N/A\"\n",
    "            print(f\"- ID {contiguous_id}: {class_name}, Color: {color}\")\n",
    "else:\n",
    "    print(\"Warning: No dataset is registered in cfg.DATASETS.TRAIN\")\n",
    "    coco_metadata = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c0107-12ec-4235-9bba-693a7e0a5e48",
   "metadata": {},
   "source": [
    "### ✅ 3. Run Panoptic Segmentation as Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c540a7-aea6-4b1d-8d0d-ccd0e4593bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Perform Panoptic Segmentation and Extract \"Stuff\" for Semantic Segmentation\n",
    "image_list = []\n",
    "for root, _, files in os.walk(input_grouped_path):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_list.append(os.path.join(root, f))\n",
    "\n",
    "total_images = len(image_list)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(image_list, desc=\"Processing images\")):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract filename and folder name\n",
    "    image_name = os.path.basename(image_path)\n",
    "    folder_name = image_name.split(\"_\")[0]  # or os.path.basename(os.path.dirname(image_path))\n",
    "    folder_path = os.path.join(output_semantic_path, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Run panoptic segmentation\n",
    "    outputs = panoptic_model(image)    \n",
    "    panoptic_seg, segments_info = outputs[\"panoptic_seg\"]\n",
    "    \n",
    "    # Convert to numpy once\n",
    "    panoptic_seg_np = panoptic_seg.cpu().numpy()\n",
    "    \n",
    "    # Generate a blank semantic segmentation mask (BGR)\n",
    "    semantic_mask = np.zeros((panoptic_seg_np.shape[0], panoptic_seg_np.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Assign COCO metadata colors\n",
    "    for segment in segments_info:\n",
    "        segment_id = segment[\"id\"]\n",
    "        category_id = segment[\"category_id\"]\n",
    "        \n",
    "        # Use numpy boolean indexing instead of comparison\n",
    "        mask = panoptic_seg_np == segment_id\n",
    "        \n",
    "        if segment[\"isthing\"]:\n",
    "            color = coco_metadata.thing_colors[category_id]\n",
    "        else:\n",
    "            color = coco_metadata.stuff_colors[category_id]\n",
    "            \n",
    "        semantic_mask[mask] = color[::-1]  # Converts to BGR\n",
    "    \n",
    "    # Save segmented image inside the grouped folder\n",
    "    output_path = os.path.join(folder_path, f\"semantic_{image_name}\")\n",
    "    cv2.imwrite(output_path, semantic_mask)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e80af-d577-4b4c-b5bf-bd121b3fda58",
   "metadata": {},
   "source": [
    "### ⛔ 4. Visualize Panoptic Segmentation (FOR TESTING PURPOSES ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fb6e2-c33d-4396-a46b-0110d878d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform panoptic segmentation\n",
    "image_list = []\n",
    "for root, _, files in os.walk(input_grouped_path):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_list.append(os.path.join(root, f))\n",
    "\n",
    "total_images = len(image_list)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(image_list, desc=\"Processing images\")):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract filename\n",
    "    image_name = os.path.basename(image_path)\n",
    "    \n",
    "    # Run panoptic segmentation\n",
    "    outputs = panoptic_model(image)    \n",
    "\n",
    "    # Confirm if output tensors are on GPU\n",
    "    for k, v in outputs.items():\n",
    "        if hasattr(v, 'device'):\n",
    "            print(f\"{k} is on {v.device}\")\n",
    "\n",
    "    # Draw panoptic segmentation image\n",
    "    panoptic_seg, segments_info = outputs[\"panoptic_seg\"]\n",
    "    v = Visualizer(image[:, :, ::-1], metadata=coco_metadata, scale=1,instance_mode=ColorMode.SEGMENTATION)\n",
    "    output_image = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info).get_image()\n",
    "\n",
    "    # Show image in Jupyter Notebook\n",
    "    print(f\"panoptic{image_name}\")\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.imshow(output_image)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae40a0-6c75-496e-aba9-184944503b1f",
   "metadata": {},
   "source": [
    "## III. DEPTH SENSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc89cb-30c6-45d1-bdb5-dea2d3beeac9",
   "metadata": {},
   "source": [
    "### ✅ 1. Save Depth Sensing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5a675-8f93-4fd5-9503-adbd2470d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DPT model\n",
    "depth_model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "depth_processor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "os.makedirs(save_instance_path, exist_ok=True)\n",
    "depth_model.save_pretrained(save_depth_path)\n",
    "depth_processor.save_pretrained(save_depth_path)\n",
    "print(f\"Model saved to {save_depth_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f9cf8-cf60-44ac-a8ff-ad41adf70b0c",
   "metadata": {},
   "source": [
    "### ✅ 2. Load Depth Sensing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c4d91-e7ee-4ab3-aaf0-beebc077da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPT model\n",
    "depth_model = DPTForDepthEstimation.from_pretrained(save_depth_path)\n",
    "depth_processor = DPTFeatureExtractor.from_pretrained(save_depth_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "depth_model.to(device)\n",
    "depth_model.eval()\n",
    "\n",
    "print(\"DPT loaded successfully!\")\n",
    "print(\"DPT is using:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffacfc-4935-41ef-b3e2-b19da5b16489",
   "metadata": {},
   "source": [
    "### ✅ 3. Run Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ecb28-1897-4fd8-9d11-8418bdd99830",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Perform Depth Estimation\n",
    "# Get all image paths from subfolders of input_grouped_path\n",
    "image_list = []\n",
    "for root, _, files in os.walk(input_grouped_path):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_list.append(os.path.join(root, f))\n",
    "\n",
    "total_images = len(image_list)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(image_list, desc=\"Processing images\")):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract filename and folder name\n",
    "    image_name = os.path.basename(image_path)\n",
    "    folder_name = image_name.split(\"_\")[0]  # or os.path.basename(os.path.dirname(image_path))\n",
    "    folder_path = os.path.join(output_depth_path, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess and move to device\n",
    "    inputs = depth_processor(images=image_rgb, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run depth estimation\n",
    "    with torch.no_grad():\n",
    "        depth_map = depth_model(**inputs).predicted_depth\n",
    "\n",
    "    # Normalize and format\n",
    "    depth_map = depth_map.squeeze().cpu().numpy()\n",
    "    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "    depth_map = (depth_map * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize to match original\n",
    "    output_image = cv2.resize(depth_map, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Save output\n",
    "    output_path = os.path.join(folder_path, f\"depth_{image_name}\")\n",
    "    cv2.imwrite(output_path, output_image)\n",
    "\n",
    "    # Optional: delete original\n",
    "    # os.remove(image_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4693a89-7032-4f73-8bb7-c61edf346c00",
   "metadata": {},
   "source": [
    "### ⛔ 4. Visualize Depth Estimation (FOR TESTING PURPOSES ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084cf14-b7af-4496-af1b-ac58d90317f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform and Visualize Depth Estimation\n",
    "# Get all image paths from subfolders of input_grouped_path\n",
    "image_list = []\n",
    "for root, _, files in os.walk(input_grouped_path):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_list.append(os.path.join(root, f))\n",
    "\n",
    "total_images = len(image_list)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(image_list, desc=\"Processing images\")):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract filename\n",
    "    image_name = os.path.basename(image_path)\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess and move to device\n",
    "    inputs = depth_processor(images=image_rgb, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run depth estimation\n",
    "    with torch.no_grad():\n",
    "        depth_map = depth_model(**inputs).predicted_depth\n",
    "\n",
    "    # Normalize and format\n",
    "    depth_map = depth_map.squeeze().cpu().numpy()\n",
    "    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "    depth_map = (depth_map * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize to match original\n",
    "    output_image = cv2.resize(depth_map, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Show image in Jupyter Notebook\n",
    "    print(f\"depth_{image_name}\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(output_image, cmap=\"magma\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2638f0-72ba-420c-8419-2154f9c50c32",
   "metadata": {},
   "source": [
    "## IV. HEIGHT, WIDTH, AND DISTANCE ESTIMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719227b5-7a38-4838-8ce7-6f2d54c30185",
   "metadata": {},
   "source": [
    "### ✅  1. Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e785197-2b02-4a29-9974-9c5bbac1ef60",
   "metadata": {},
   "source": [
    "a. Converting hexcode to rgb tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0bda5-0a8e-4a4e-9775-6dd54c990a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_color):\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf4e4a-cfac-4624-8d24-50316811ab94",
   "metadata": {},
   "source": [
    "b. Post-process semantic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946f031-69a1-4acf-9b54-e7f6f49ce267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_thin_shape(component_mask, max_thickness=2):\n",
    "    mask = (component_mask > 0).astype(np.uint8)\n",
    "    thickness = 0\n",
    "    while np.any(mask):\n",
    "        mask = cv2.erode(mask, np.ones((3, 3), np.uint8))\n",
    "        thickness += 1\n",
    "        if thickness > max_thickness:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de254d3f-6456-4979-924c-a9c0d48452b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_to_black(img, color_map, classes_to_remove):\n",
    "    img_out = img.copy()\n",
    "\n",
    "    for class_name in classes_to_remove:\n",
    "        if class_name not in color_map:\n",
    "            continue\n",
    "        rgb = np.array(hex_to_rgb(color_map[class_name]))\n",
    "        mask = np.all(img_out == rgb, axis=2)\n",
    "        img_out[mask] = [0, 0, 0]  # Set to black\n",
    "\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed77d88-0e93-492e-88d5-411ca75b7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_semantic(img_rgb, color_map, tolerance=40, min_area=30, max_thickness=3):\n",
    "    h, w, _ = img_rgb.shape\n",
    "    cleaned = np.zeros_like(img_rgb)\n",
    "\n",
    "    # Convert hex colors to RGB array\n",
    "    class_items = list(color_map.items())\n",
    "    class_colors = [np.array(hex_to_rgb(hex_color)) for _, hex_color in class_items]\n",
    "    class_colors = np.array(class_colors)  # shape: (num_classes, 3)\n",
    "\n",
    "    # Flatten image\n",
    "    img_flat = img_rgb.reshape(-1, 3)\n",
    "\n",
    "    # Compute color differences\n",
    "    diffs = np.abs(img_flat[:, None, :] - class_colors[None, :, :])  # (N, C, 3)\n",
    "    color_diffs = np.sum(diffs, axis=2)  # (N, C)\n",
    "    min_diff = np.min(color_diffs, axis=1)\n",
    "    min_idx = np.argmin(color_diffs, axis=1)\n",
    "\n",
    "    # Assign class color if within tolerance\n",
    "    cleaned_flat = cleaned.reshape(-1, 3)\n",
    "    valid = min_diff < tolerance\n",
    "    cleaned_flat[valid] = class_colors[min_idx[valid]]\n",
    "\n",
    "    cleaned = cleaned.reshape(h, w, 3)\n",
    "\n",
    "    # Remove small or thin structures for each class\n",
    "    for color in class_colors:\n",
    "        mask = np.all(cleaned == color, axis=2).astype(np.uint8)\n",
    "        num, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "\n",
    "        for i in range(1, num):  # skip background\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            component_mask = (labels == i).astype(np.uint8)\n",
    "\n",
    "            if area < min_area or is_thin_shape(component_mask, max_thickness=max_thickness):\n",
    "                cleaned[labels == i] = [0, 0, 0]  # replace with black\n",
    "\n",
    "    # Make awning, canopy, bridge color black\n",
    "    classes_to_be_removed = ['awning', 'canopy', 'bridge']\n",
    "    final = classes_to_black(cleaned, color_map, classes_to_be_removed)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d881dd-9dd6-4d47-898d-751a8d9360c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint_semantic(mask_rgb, vanishing_lines, color_map, max_iters=1000):\n",
    "    filled = mask_rgb.copy()\n",
    "    H, W = filled.shape[:2]\n",
    "\n",
    "    # Encode image\n",
    "    color_encoded = (filled[:, :, 0].astype(np.int32) << 16) + \\\n",
    "                    (filled[:, :, 1].astype(np.int32) << 8) + \\\n",
    "                    filled[:, :, 2].astype(np.int32)\n",
    "\n",
    "    black_mask = (color_encoded == 0)\n",
    "\n",
    "    # === Stage 1: Pre-fill black pixels above sky in vanishing regions ===\n",
    "    sky_rgb = hex_to_rgb(color_map['sky'])\n",
    "    sky_val = (sky_rgb[0] << 16) + (sky_rgb[1] << 8) + sky_rgb[2]\n",
    "\n",
    "    lines_sorted = sorted(vanishing_lines, key=lambda line: line[0][0])\n",
    "    for i in range(len(lines_sorted) - 1):\n",
    "        (x1a, y1a), (x1b, y1b) = lines_sorted[i]\n",
    "        (x2a, y2a), (x2b, y2b) = lines_sorted[i + 1]\n",
    "\n",
    "        poly = np.array([[x1a, y1a], [x1b, y1b], [x2b, y2b], [x2a, y2a]], dtype=np.int32)\n",
    "        region_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        cv2.fillPoly(region_mask, [poly], 1)\n",
    "\n",
    "        # Fill sky pixels upward\n",
    "        region = (region_mask == 1)\n",
    "        sky_pixels = np.logical_and(region, color_encoded == sky_val)\n",
    "        ys, xs = np.where(sky_pixels)\n",
    "        for x, y in zip(xs, ys):\n",
    "            for y_up in range(y - 1, -1, -1):\n",
    "                if not region_mask[y_up, x] or color_encoded[y_up, x] != 0:\n",
    "                    break\n",
    "                color_encoded[y_up, x] = sky_val\n",
    "\n",
    "    # === Stage 2: Pre-fill black pixels below floor in vanishing regions ===\n",
    "    floor_classes = [\n",
    "        'sidewalk', 'pavement', 'road', 'route', 'floor', 'flooring',\n",
    "        'ground', 'earth', 'field', 'runway', 'land', 'soil',\n",
    "        'sand', 'dirt', 'track', 'path', 'rock', 'stone'\n",
    "    ]\n",
    "    floor_colors = [hex_to_rgb(color_map[c]) for c in floor_classes if c in color_map]\n",
    "    floor_vals = set((r << 16) + (g << 8) + b for r, g, b in floor_colors)\n",
    "\n",
    "    for i in range(len(lines_sorted) - 1):\n",
    "        (x1a, y1a), (x1b, y1b) = lines_sorted[i]\n",
    "        (x2a, y2a), (x2b, y2b) = lines_sorted[i + 1]\n",
    "\n",
    "        poly = np.array([[x1a, y1a], [x1b, y1b], [x2b, y2b], [x2a, y2a]], dtype=np.int32)\n",
    "        region_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        cv2.fillPoly(region_mask, [poly], 1)\n",
    "\n",
    "        # Fill floor pixels downward\n",
    "        region = (region_mask == 1)\n",
    "        floor_pixels = np.logical_and(region, np.isin(color_encoded, list(floor_vals)))\n",
    "        ys, xs = np.where(floor_pixels)\n",
    "        for x, y in zip(xs, ys):\n",
    "            val = color_encoded[y, x]\n",
    "            for y_down in range(y + 1, H):\n",
    "                if not region_mask[y_down, x] or color_encoded[y_down, x] != 0:\n",
    "                    break\n",
    "                color_encoded[y_down, x] = val\n",
    "\n",
    "    # === Stage 3: General isotropic inpainting ===\n",
    "    black_mask = (color_encoded == 0)\n",
    "    for _ in range(max_iters):\n",
    "        if not np.any(black_mask):\n",
    "            break\n",
    "        dilated = grey_dilation(color_encoded, size=(3, 3))\n",
    "        replacement_mask = black_mask & (dilated != 0)\n",
    "        color_encoded[replacement_mask] = dilated[replacement_mask]\n",
    "        black_mask = (color_encoded == 0)\n",
    "\n",
    "    # Decode to RGB\n",
    "    r = (color_encoded >> 16) & 255\n",
    "    g = (color_encoded >> 8) & 255\n",
    "    b = color_encoded & 255\n",
    "    result_rgb = np.stack([r, g, b], axis=2).astype(np.uint8)\n",
    "\n",
    "    return result_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_ceiling(img):\n",
    "    # Load image and convert to RGB\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Get the top 1/4 of the image\n",
    "    height = img_np.shape[0]\n",
    "    top_quarter = img_np[:height // 4, :, :]\n",
    "\n",
    "    # Create mask for black pixels\n",
    "    black_pixels = np.all(top_quarter == [0, 0, 0], axis=-1)\n",
    "\n",
    "    # Calculate the percentage of black pixels\n",
    "    black_ratio = np.sum(black_pixels) / black_pixels.size\n",
    "\n",
    "    # Return True if black pixels make up at least 20%\n",
    "    return black_ratio >= 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_ceiling_2(img, color_map, vanishing_lines):\n",
    "    H, W = img.shape[:2]\n",
    "    \n",
    "    structure_classes = {'building', 'edifice', 'house', 'skyscraper', 'tower'}\n",
    "    wall_classes = {'wall'}\n",
    "    relevant_classes = structure_classes | wall_classes\n",
    "\n",
    "    # Build RGB -> label map\n",
    "    color_to_label = {}\n",
    "    label = 1\n",
    "    for cls, hex_color in color_map.items():\n",
    "        rgb = hex_to_rgb(hex_color)\n",
    "        color_to_label[rgb] = label\n",
    "        label += 1\n",
    "\n",
    "    # Build label -> class name map\n",
    "    rgb_to_class = {hex_to_rgb(v): k for k, v in color_map.items()}\n",
    "    sky_rgb = hex_to_rgb(color_map['sky'])\n",
    "\n",
    "    # Convert image to label mask\n",
    "    label_img = np.zeros((H, W), dtype=np.uint16)\n",
    "    for rgb, label_val in color_to_label.items():\n",
    "        match_mask = np.all(img == rgb, axis=2)\n",
    "        label_img[match_mask] = label_val\n",
    "\n",
    "    # Get labels for relevant structure/wall classes\n",
    "    relevant_labels = {\n",
    "        color_to_label[hex_to_rgb(color_map[cls])]\n",
    "        for cls in relevant_classes if cls in color_map\n",
    "    }\n",
    "\n",
    "    sky_label = color_to_label[sky_rgb]\n",
    "    total_sky = np.sum(label_img == sky_label)\n",
    "    if total_sky == 0:\n",
    "        return False\n",
    "\n",
    "    # Sort vanishing lines\n",
    "    lines_sorted = sorted(vanishing_lines, key=lambda line: line[0][0])\n",
    "\n",
    "    sky_below_total = 0\n",
    "    for i in range(len(lines_sorted) - 1):\n",
    "        (x1a, y1a), (x1b, y1b) = lines_sorted[i]\n",
    "        (x2a, y2a), (x2b, y2b) = lines_sorted[i + 1]\n",
    "\n",
    "        poly = np.array([[x1a, y1a], [x1b, y1b], [x2b, y2b], [x2a, y2a]], dtype=np.int32)\n",
    "        region_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        cv2.fillPoly(region_mask, [poly], 1)\n",
    "\n",
    "        # Restrict label image to region\n",
    "        region_labels = np.where(region_mask == 1, label_img, 0)\n",
    "\n",
    "        # Find the highest y (min y) where a structure/wall label exists\n",
    "        struct_ys, _ = np.where(np.isin(region_labels, list(relevant_labels)))\n",
    "        if struct_ys.size == 0:\n",
    "            continue\n",
    "        y_top = struct_ys.min()\n",
    "\n",
    "        # Find sky pixels in region *below* y_top\n",
    "        region_sky_mask = np.logical_and(label_img == sky_label, region_mask == 1)\n",
    "        sky_ys, _ = np.where(region_sky_mask)\n",
    "        sky_below = np.sum(sky_ys > y_top)\n",
    "        sky_below_total += sky_below\n",
    "\n",
    "    return (sky_below_total / total_sky) >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fe8b4-7d62-434c-ac06-0816f5968513",
   "metadata": {},
   "source": [
    "c. Creating masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157300fd-5511-45d0-b551-d64aeca00a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask(mask, min_area=50, kernel_size=2):\n",
    "    # Morphological opening\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    opened  = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Connected‑component filtering \n",
    "    num, labels, stats, _ = cv2.connectedComponentsWithStats(opened, connectivity=8)\n",
    "    cleaned = np.zeros_like(mask)\n",
    "\n",
    "    for i in range(1, num):\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        if area >= min_area:\n",
    "            cleaned[labels == i] = 255\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa9fa7-fa84-4701-a8c5-5f02da69d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(semantic_img, class_labels, color_map, tolerance=40): # tolerance of 20\n",
    "    mask = np.zeros(semantic_img.shape[:2], dtype=np.uint8)\n",
    "    for class_label in class_labels:\n",
    "        target_color = np.array(hex_to_rgb(color_map[class_label]))\n",
    "        diff = np.abs(semantic_img - target_color)\n",
    "        within_tol = np.all(diff < tolerance, axis=2)\n",
    "        mask[within_tol] = 255\n",
    "    return clean_mask(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac2c8f-528b-4828-854b-fad7939b04b1",
   "metadata": {},
   "source": [
    "d. Extracting objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd2442-f2b8-46d1-905c-8d2d8aa4a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects(mask, depth, min_area=600):\n",
    "    object_masks = []\n",
    "\n",
    "    # Ensure binary mask\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Connected component analysis\n",
    "    num_labels, labels = cv2.connectedComponents(binary_mask, connectivity=8)\n",
    "\n",
    "    # Skip background (label 0)\n",
    "    for i in range(1, num_labels):\n",
    "        obj_mask = (labels == i).astype(np.uint8) * 255\n",
    "\n",
    "        # Filter out small objects\n",
    "        if cv2.countNonZero(obj_mask) >= min_area:\n",
    "            object_masks.append(obj_mask)\n",
    "\n",
    "    return object_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6edb39-e36a-4e65-9c0e-751607dab508",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def extract_objects2(mask, depth, depth_cluster_thresh=5.0, min_split_area=2000):\n",
    "    object_masks = []\n",
    "\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "    num_labels, labels = cv2.connectedComponents(binary_mask, connectivity=8)\n",
    "\n",
    "    for i in range(1, num_labels):  # Skip background\n",
    "        obj_mask = (labels == i).astype(np.uint8)\n",
    "        masked_depth = depth[obj_mask > 0]\n",
    "\n",
    "        if len(masked_depth) < 50:\n",
    "            continue  # skip very small objects\n",
    "\n",
    "        # Depth-based clustering\n",
    "        kmeans = KMeans(n_clusters=2, n_init=\"auto\")\n",
    "        depths = masked_depth.reshape(-1, 1)\n",
    "        kmeans.fit(depths)\n",
    "        centers = kmeans.cluster_centers_.flatten()\n",
    "        depth_diff = np.abs(centers[0] - centers[1])\n",
    "\n",
    "        if depth_diff > depth_cluster_thresh:\n",
    "            # Split into two masks\n",
    "            full_indices = np.where(obj_mask > 0)\n",
    "            cluster_labels = kmeans.labels_\n",
    "\n",
    "            mask1 = np.zeros_like(obj_mask, dtype=np.uint8)\n",
    "            mask2 = np.zeros_like(obj_mask, dtype=np.uint8)\n",
    "\n",
    "            for j in range(len(cluster_labels)):\n",
    "                y, x = full_indices[0][j], full_indices[1][j]\n",
    "                if cluster_labels[j] == 0:\n",
    "                    mask1[y, x] = 255\n",
    "                else:\n",
    "                    mask2[y, x] = 255\n",
    "\n",
    "            if np.sum(mask1) > min_split_area:\n",
    "                object_masks.append(mask1)\n",
    "            if np.sum(mask2) > min_split_area:\n",
    "                object_masks.append(mask2)\n",
    "        else:\n",
    "            object_masks.append(obj_mask * 255)\n",
    "\n",
    "    return object_masks\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae3fb2-8551-477d-8c2c-4305c1ea35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def extract_objects3(mask, depth, expected_count):\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "    num_labels, labels = cv2.connectedComponents(binary_mask, connectivity=8)\n",
    "\n",
    "    depth_points = []\n",
    "    pixel_coords = []\n",
    "\n",
    "    for i in range(1, num_labels):  # Skip background\n",
    "        obj_mask = (labels == i).astype(np.uint8)\n",
    "        ys, xs = np.where(obj_mask)\n",
    "\n",
    "        for y, x in zip(ys, xs):\n",
    "            d = depth[y, x]\n",
    "            if np.isfinite(d):\n",
    "                depth_points.append([d])\n",
    "                pixel_coords.append((y, x))\n",
    "\n",
    "    if len(depth_points) < expected_count:\n",
    "        return []  # not enough data to cluster\n",
    "\n",
    "    kmeans = KMeans(n_clusters=expected_count, n_init=\"auto\")\n",
    "    kmeans.fit(depth_points)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    masks = [np.zeros_like(mask, dtype=np.uint8) for _ in range(expected_count)]\n",
    "    for (y, x), label in zip(pixel_coords, cluster_labels):\n",
    "        masks[label][y, x] = 255\n",
    "\n",
    "    final_objects = []\n",
    "    for cluster_mask in masks:\n",
    "        # Get connected components and keep only the largest\n",
    "        num_labels, labels = cv2.connectedComponents(cluster_mask, connectivity=8)\n",
    "\n",
    "        max_area = 0\n",
    "        max_mask = None\n",
    "        for i in range(1, num_labels):\n",
    "            component_mask = (labels == i).astype(np.uint8) * 255\n",
    "            area = np.sum(component_mask > 0)\n",
    "            if area > max_area and area > min_split_area:\n",
    "                max_area = area\n",
    "                max_mask = component_mask\n",
    "\n",
    "        if max_mask is not None:\n",
    "            final_objects.append(max_mask)\n",
    "\n",
    "    return final_objects\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9e65f-3198-41e2-bca6-8f3ba6a46e8b",
   "metadata": {},
   "source": [
    "e. Creating vanishing lines and dividing objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98345d3d-fba9-44a0-a84a-08ed5daa4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vanishing_lines(img, num_lines=100, extra_side_lines=30):\n",
    "    vanishing_lines = []\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    h, w = gray.shape\n",
    "\n",
    "    # Line Segment Detection\n",
    "    lsd = cv2.createLineSegmentDetector(0)\n",
    "    lines, _, _, _ = lsd.detect(gray)\n",
    "\n",
    "    # Filter near-vertical lines\n",
    "    vertical_lines = []\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        angle = np.degrees(np.arctan2(dy, dx)) % 180\n",
    "        if 75 <= angle <= 105:  # near-vertical\n",
    "            vertical_lines.append([x1, y1, x2, y2])\n",
    "\n",
    "    # Estimate vertical vanishing point\n",
    "    lines_h = []\n",
    "    for x1, y1, x2, y2 in vertical_lines:\n",
    "        p1 = np.array([x1, y1, 1.0])\n",
    "        p2 = np.array([x2, y2, 1.0])\n",
    "        line_h = np.cross(p1, p2)\n",
    "        lines_h.append(line_h)\n",
    "\n",
    "    if len(lines_h) >= 2:\n",
    "        A = np.stack(lines_h)\n",
    "        _, _, Vt = np.linalg.svd(A)\n",
    "        vp_h = Vt[-1]\n",
    "        vp = vp_h[:2] / vp_h[2]\n",
    "    else:\n",
    "        vp = np.array([w // 2, 0])  # fallback\n",
    "\n",
    "    # Get bottom-to-vanishing-point lines\n",
    "    step = w // num_lines\n",
    "    for i in range(num_lines):\n",
    "        x = i * step + step // 2\n",
    "        pt1 = (int(x), h)\n",
    "        pt2 = (int(vp[0]), int(vp[1]))\n",
    "        vanishing_lines.append((pt1, pt2))\n",
    "\n",
    "    # Get side-to-vanishing-point lines from left and right edges\n",
    "    y_steps = np.linspace(0, h, extra_side_lines, endpoint=False).astype(int)\n",
    "    for y in y_steps:\n",
    "        # From left edge\n",
    "        pt1 = (0, y)\n",
    "        pt2 = (int(vp[0]), int(vp[1]))\n",
    "        vanishing_lines.append((pt1, pt2))\n",
    "\n",
    "        # From right edge\n",
    "        pt1 = (w - 1, y)\n",
    "        vanishing_lines.append((pt1, pt2))\n",
    "\n",
    "    return vanishing_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3ab6e-535b-43e4-8095-044aa6ae347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_object_by_vanishing_lines(obj_mask, vanishing_lines, img_shape, top_y_jump_thresh=20):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    H, W = img_shape[:2]\n",
    "    masks = []\n",
    "\n",
    "    # 1. Sort vanishing lines by x of the bottom point (pt1)\n",
    "    lines_sorted = sorted(vanishing_lines, key=lambda line: line[0][0])\n",
    "\n",
    "    # 2. Create polygonal region masks\n",
    "    region_masks = []\n",
    "    for i in range(len(lines_sorted) - 1):\n",
    "        (x1a, y1a), (x1b, y1b) = lines_sorted[i]\n",
    "        (x2a, y2a), (x2b, y2b) = lines_sorted[i + 1]\n",
    "\n",
    "        poly = np.array([[x1a, y1a], [x1b, y1b], [x2b, y2b], [x2a, y2a]], dtype=np.int32)\n",
    "        region_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        cv2.fillPoly(region_mask, [poly], 1)\n",
    "        region_masks.append(region_mask)\n",
    "\n",
    "    # 3. Compute top Y per region\n",
    "    region_top_ys = []\n",
    "    for region_mask in region_masks:\n",
    "        masked = obj_mask * region_mask\n",
    "        ys, xs = np.where(masked)\n",
    "\n",
    "        if len(ys) == 0:\n",
    "            region_top_ys.append(None)\n",
    "        else:\n",
    "            top_y = np.mean(ys[ys == np.min(ys)])  # average of top-row pixels\n",
    "            region_top_ys.append(top_y)\n",
    "\n",
    "    # 4. Determine valid splits (large top y jump)\n",
    "    tentative_splits = []\n",
    "    for i in range(len(region_top_ys) - 1):\n",
    "        if region_top_ys[i] is None or region_top_ys[i + 1] is None:\n",
    "            continue\n",
    "\n",
    "        y_diff = abs(region_top_ys[i] - region_top_ys[i + 1])\n",
    "        if y_diff > top_y_jump_thresh:\n",
    "            tentative_splits.append(i + 1)\n",
    "\n",
    "    # 5. Remove splits that would isolate a single column\n",
    "    final_splits = []\n",
    "    for i in range(len(tentative_splits)):\n",
    "        curr = tentative_splits[i]\n",
    "\n",
    "        prev_split = tentative_splits[i - 1] if i > 0 else -1\n",
    "        next_split = tentative_splits[i + 1] if i + 1 < len(tentative_splits) else len(region_masks)\n",
    "\n",
    "        # Only allow if it won’t isolate a single region\n",
    "        if next_split - prev_split > 2:\n",
    "            final_splits.append(curr)\n",
    "\n",
    "    # 6. Group regions by split boundaries\n",
    "    split_boundaries = final_splits + [len(region_masks)]\n",
    "    prev = 0\n",
    "    for idx in split_boundaries:\n",
    "        combined_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        for i in range(prev, idx):\n",
    "            combined_mask |= (region_masks[i] & obj_mask)\n",
    "        if np.any(combined_mask):\n",
    "            masks.append(combined_mask)\n",
    "        prev = idx\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71399a-f8a3-413c-bda4-27b4d95f1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_visualization(mask, objects, alpha=0.6):\n",
    "    if mask.ndim == 2:\n",
    "        mask_rgb = cv2.cvtColor((mask * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    else:\n",
    "        mask_rgb = mask.copy()\n",
    "\n",
    "    overlay = mask_rgb.copy()\n",
    "\n",
    "    for obj_mask in objects:\n",
    "        obj_mask_uint8 = (obj_mask > 0).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(obj_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        color = [random.randint(100, 255) for _ in range(3)]\n",
    "        cv2.drawContours(overlay, contours, -1, color, thickness=1)\n",
    "\n",
    "    blended = cv2.addWeighted(overlay, 1, mask_rgb, alpha, 0)\n",
    "    result_image = cv2.cvtColor(blended, cv2.COLOR_BGR2RGB)\n",
    "    return result_image, len(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfde8e6-1755-4952-9bfd-d3013e6f2b32",
   "metadata": {},
   "source": [
    "f. Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3962cd-3b74-4672-b2b2-7958fdb2be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(tuples, grid_shape=(3, 4), figsize=(16, 6)):\n",
    "    rows, cols = grid_shape\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for title, image, cmap, row, col in tuples:\n",
    "        ax = plt.subplot2grid((rows, cols), (row, col))\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc591c-6cca-4d2f-bf9a-1507b1adc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vanishing_lines(img, vanishing_lines, color=(0, 0, 255), thickness=1):\n",
    "    # Generate image with vanishing lines\n",
    "    vanishing_lines_img = img.copy()\n",
    "    for pt1, pt2 in vanishing_lines:\n",
    "        cv2.line(vanishing_lines_img, pt1, pt2, color, thickness)\n",
    "\n",
    "    return vanishing_lines_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91625b38-d5a9-4452-a16a-ece54d08096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_objects(objects, title_prefix=\"Object\", cols=5, cmap='gray'):\n",
    "    rows = (len(objects) + cols - 1) // cols\n",
    "    plt.figure(figsize=(3 * cols, 3 * rows))\n",
    "\n",
    "    for i, obj_mask in enumerate(objects):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(obj_mask, cmap=cmap)\n",
    "        plt.title(f\"{title_prefix} {i+1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9549c-29df-4a90-9aad-6caa2cac85d5",
   "metadata": {},
   "source": [
    "g. JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c4d9c-ead7-46dd-abde-0830c8355557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_objects_to_json(json_path, all_objects_info):\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(all_objects_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e1a01-0be0-437a-aca2-4887187c0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def touches_image_edge(mask):\n",
    "    h, w = mask.shape\n",
    "    top_edge = np.any(mask[0, :])\n",
    "    bottom_edge = np.any(mask[-1, :])\n",
    "    left_edge = np.any(mask[:, 0])\n",
    "    right_edge = np.any(mask[:, -1])\n",
    "    \n",
    "    edges = []\n",
    "    if top_edge: edges.append(\"top\")\n",
    "    if bottom_edge: edges.append(\"bottom\")\n",
    "    if left_edge: edges.append(\"left\")\n",
    "    if right_edge: edges.append(\"right\")\n",
    "\n",
    "    return edges  # returns list like ['top', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a394b-42bd-48ab-8036-134bc65060a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_metadata(\n",
    "    coord_angle,\n",
    "    obj,\n",
    "    object_type,\n",
    "    avg_depth,\n",
    "    pixel_height,\n",
    "):\n",
    "    # DISTANCE (in meters)\n",
    "    distance = 800 / avg_depth\n",
    "\n",
    "    # HEIGHT (in meters)\n",
    "    height = 4.00 * ((pixel_height / avg_depth) ** 1.27) if avg_depth > 0 else 0\n",
    "\n",
    "    # WIDTH (in meters)\n",
    "    ys, xs = np.nonzero(obj)\n",
    "    if xs.size > 0 and pixel_height > 0:\n",
    "        pixel_width = xs.max() - xs.min()\n",
    "        scale = height / pixel_height\n",
    "        width = pixel_width * scale\n",
    "    else:\n",
    "        width = 0\n",
    "\n",
    "    # HORIZONTAL OFFSET: -100 (left), 0 (center), +100 (right)\n",
    "    if xs.size > 0:\n",
    "        _, image_width = obj.shape\n",
    "        object_center_x = (xs.max() + xs.min()) / 2\n",
    "        image_center_x = image_width / 2\n",
    "        horizontal_offset = 200 * (object_center_x - image_center_x) / image_width\n",
    "    else:\n",
    "        horizontal_offset = 0\n",
    "\n",
    "    # ISCUT (top/bottom/left/right)\n",
    "    is_cut = touches_image_edge(obj)\n",
    "\n",
    "    return {\n",
    "        \"fileName\": coord_angle,\n",
    "        \"objectType\": object_type,\n",
    "        \"distance\": distance,\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"horizontalOffset\": horizontal_offset,\n",
    "        \"isCut\": is_cut\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985bc05-478a-4025-a52a-bb801f909988",
   "metadata": {},
   "source": [
    "h. Getting average depth and pixel height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d24007-aa80-45bd-abcd-5755678232d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(mask, semantic_rgb, depth_map, color_map, interested_classes):\n",
    "    # Convert class hex colors to RGB arrays\n",
    "    interested_colors = [np.array(hex_to_rgb(color_map[c])) for c in interested_classes]\n",
    "\n",
    "    region_mask = mask > 0\n",
    "    depth_values = []\n",
    "\n",
    "    # Accumulator for combined mask\n",
    "    accumulated_mask = np.zeros(mask.shape, dtype=np.uint8)\n",
    "\n",
    "    for color in interested_colors:\n",
    "        # Exact color match in semantic segmentation\n",
    "        match_mask = np.all(semantic_rgb == color, axis=2)\n",
    "        combined_mask = match_mask & region_mask\n",
    "\n",
    "        # Add to accumulated mask for visualization\n",
    "        accumulated_mask |= combined_mask.astype(np.uint8)\n",
    "\n",
    "        # Collect depth values\n",
    "        values = depth_map[combined_mask]\n",
    "        if values.size > 0:\n",
    "            depth_values.extend(values.tolist())\n",
    "\n",
    "    depth_values = np.array(depth_values)\n",
    "    depth_values = depth_values[np.isfinite(depth_values) & (depth_values > 0)]\n",
    "\n",
    "    # If none, return none\n",
    "    if depth_values.size == 0:\n",
    "        return None, accumulated_mask\n",
    "\n",
    "    # If less than 5 or everything is almost the same, return mean\n",
    "    if depth_values.size < 5 or np.std(depth_values) < 1e-3:\n",
    "        return float(np.mean(depth_values)), accumulated_mask\n",
    "\n",
    "    # If more than enough, perform outlier filtering\n",
    "    if depth_values.size >= 5:\n",
    "        lower, upper = np.percentile(depth_values, [10, 90])\n",
    "        filtered = depth_values[(depth_values >= lower) & (depth_values <= upper)]\n",
    "        if filtered.size >= 1:\n",
    "            depth_values = filtered\n",
    "    try:\n",
    "        kde = gaussian_kde(depth_values)\n",
    "        xs = np.linspace(depth_values.min(), depth_values.max(), 500)\n",
    "        density = kde(xs)\n",
    "        mode_depth = xs[np.argmax(density)]\n",
    "        return float(mode_depth), accumulated_mask\n",
    "    except np.linalg.LinAlgError:\n",
    "        return float(np.mean(depth_values)), accumulated_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08ec80-da7f-466a-98b6-9307f8cb49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_height(mask, vanishing_lines):\n",
    "\n",
    "    h, w = mask.shape\n",
    "    mask = mask.astype(np.uint8)\n",
    "\n",
    "    distances = []\n",
    "    edge_distances = []\n",
    "\n",
    "    for i in range(len(vanishing_lines) - 1):\n",
    "        pt1a, pt2a = vanishing_lines[i]\n",
    "        pt1b, pt2b = vanishing_lines[i + 1]\n",
    "\n",
    "        # Create polygon between two rays (vanishing lines)\n",
    "        polygon = np.array([pt1a, pt2a, pt2b, pt1b], dtype=np.int32)\n",
    "\n",
    "        # Create binary mask of that polygon\n",
    "        region_mask = np.zeros_like(mask)\n",
    "        cv2.fillPoly(region_mask, [polygon], 1)\n",
    "\n",
    "        # Get intersection of region and object mask\n",
    "        intersect = (mask & region_mask).astype(np.uint8)\n",
    "\n",
    "        # Get coordinates of nonzero pixels\n",
    "        ys, xs = np.nonzero(intersect)\n",
    "        if len(xs) == 0:\n",
    "            continue\n",
    "\n",
    "        points = np.stack([xs, ys], axis=1)\n",
    "        top = points[np.argmin(points[:, 1])]\n",
    "        bottom = points[np.argmax(points[:, 1])]\n",
    "        dist = np.linalg.norm(top - bottom)\n",
    "\n",
    "        # Skip if intersection touches any edge of the image\n",
    "        if (\n",
    "            np.any(intersect[0, :]) or     # top edge\n",
    "            np.any(intersect[-1, :]) or    # bottom edge\n",
    "            np.any(intersect[:, 0]) or     # left edge\n",
    "            np.any(intersect[:, -1])       # right edge\n",
    "        ):\n",
    "            edge_distances.append(dist)\n",
    "        else:            \n",
    "            distances.append(dist)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    edge_distances = np.array(edge_distances)\n",
    "\n",
    "    # If all columns touch the edge, return highest from edge distances\n",
    "    # If no columns at all, return None\n",
    "    if distances.size == 0:\n",
    "        return max(edge_distances) if edge_distances.size > 0 else None\n",
    "\n",
    "    # If less than 5 columns not touching the edge, return mean\n",
    "    if distances.size < 5:\n",
    "        return np.mean(distances)\n",
    "    \n",
    "    # If more than enough, perform outlier filtering\n",
    "    if distances.size >= 5:\n",
    "        lower, upper = np.percentile(distances, [10, 90])\n",
    "        filtered = distances[(distances >= lower) & (distances <= upper)]\n",
    "        return np.mean(filtered) if filtered.size > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4673ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bottom_y(obj):\n",
    "    ys, xs = np.where(obj > 0)\n",
    "    if len(ys) == 0:\n",
    "        return None, None\n",
    "\n",
    "    top_y = ys.min()\n",
    "    bottom_y = ys.max()\n",
    "    return top_y, bottom_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95ffba-3d41-41f9-acad-58ca902f24b2",
   "metadata": {},
   "source": [
    "### ✅ 2. Store Class-Color Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65e231-bd0d-45eb-99f7-17c966f1c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store color map from text file to variable\n",
    "color_map = {}\n",
    "\n",
    "with open(color_map_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():  # skip empty lines\n",
    "            color_hex, labels = line.strip().split(\"\\t\")\n",
    "            class_names = labels.split(\";\")\n",
    "            for class_name in class_names:\n",
    "                color_map[class_name.strip()] = color_hex.strip()\n",
    "\n",
    "# Number of unique classes\n",
    "num_classes = len(color_map)\n",
    "\n",
    "# Number of unique hex colors\n",
    "unique_colors = set(color_map.values())\n",
    "num_colors = len(unique_colors)\n",
    "\n",
    "print(f\"Number of unique classes: {num_classes}\")\n",
    "print(f\"Number of unique hex colors: {num_colors}\")\n",
    "\n",
    "display(color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b728d8d-58f6-42b3-89d8-b6c5efdccc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class groups\n",
    "structure_classes = ['building', 'edifice', 'house', 'skyscraper', 'tower']\n",
    "wall_classes = ['wall']\n",
    "overhead_classes = ['awning', 'canopy', 'bridge']\n",
    "vegetation_classes = ['tree', 'palm tree']\n",
    "base_classes = [\n",
    "    'sidewalk', 'pavement', 'road', 'route', 'floor', 'flooring',\n",
    "    'ground', 'earth', 'field', 'runway', 'land', 'soil',\n",
    "    'sand', 'dirt', 'track', 'path', 'rock', 'stone', 'sky'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72219839-90f1-4612-9676-9c1c79f33b8a",
   "metadata": {},
   "source": [
    "### ✅ 3. Create vanishing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579ae16-00b6-4768-93a1-760d720d4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img_path = os.path.join(input_vanishing_path, '14.558112,121.0256685_45.jpg')\n",
    "ref_img = cv2.imread(ref_img_path)\n",
    "vanishing_lines = get_vanishing_lines(ref_img)\n",
    "\n",
    "ref_vanishing_img = display_vanishing_lines(ref_img, vanishing_lines)\n",
    "ref_vanishing_img_rgb = cv2.cvtColor(ref_vanishing_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display with matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(ref_vanishing_img_rgb)\n",
    "plt.title(\"Vanishing Lines\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac94ab-ddad-40ca-9be2-b31d0c3ff6bf",
   "metadata": {},
   "source": [
    "### ⛔ 4. Create masks for each class, separate them as objects, record their data in JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d67dc2-713c-493e-b09a-ddf76c3e105d",
   "metadata": {},
   "source": [
    "a. Creating JSON files for each coordinate angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88938d-8b10-43ba-a93f-85987389fdfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_objects_info = []\n",
    "\n",
    "# Traverse coordinate folders\n",
    "for folder_name in tqdm(sorted(os.listdir(input_grouped_path)), desc=\"Processing folders\"):\n",
    "    folder_input = os.path.join(input_grouped_path, folder_name)\n",
    "    folder_depth = os.path.join(output_depth_path, folder_name)\n",
    "    folder_semantic = os.path.join(output_semantic_path, folder_name)\n",
    "\n",
    "    if not os.path.isdir(folder_input):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(folder_input):\n",
    "        if not filename.endswith(\".jpg\"):\n",
    "            continue\n",
    "        \n",
    "        # Parse base filename\n",
    "        base_name = filename.replace(\".jpg\", \"\")\n",
    "        coord_angle = base_name  # example: \"14.55208,121.0210221_0\"\n",
    "        folder_output_json = os.path.join(output_json_path, f\"{coord_angle}.json\")\n",
    "        if os.path.exists(folder_output_json):\n",
    "            continue\n",
    "\n",
    "        # Build full paths\n",
    "        img_path = os.path.join(folder_input, filename)\n",
    "        sem_path = os.path.join(folder_semantic, f\"semantic_{coord_angle}.jpg\")\n",
    "        depth_path = os.path.join(folder_depth, f\"depth_{coord_angle}.jpg\")\n",
    "\n",
    "        # Check if all files exist\n",
    "        if not (os.path.exists(img_path) and os.path.exists(sem_path) and os.path.exists(depth_path)):\n",
    "            continue\n",
    "\n",
    "        # print(coord_angle)\n",
    "        \n",
    "        # Load semantic image\n",
    "        semantic_bgr = cv2.imread(sem_path)\n",
    "        semantic_rgb = cv2.cvtColor(semantic_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Inpaint semantic image (skip if there's a ceiling)\n",
    "        cleaned_semantic = clean_semantic(semantic_rgb, color_map)\n",
    "        # IF black ceiling\n",
    "        if (has_ceiling(cleaned_semantic)):\n",
    "            continue\n",
    "        # IF structure/wall ceiling\n",
    "        if (has_ceiling_2(cleaned_semantic, color_map, vanishing_lines)):\n",
    "            continue\n",
    "        inpainted_semantic = inpaint_semantic(cleaned_semantic, vanishing_lines, color_map)\n",
    "\n",
    "        # Load original image, depth image, vanishing lines image\n",
    "        img = cv2.imread(img_path)\n",
    "        depth = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n",
    "        vanishing_lines_img = display_vanishing_lines(img, vanishing_lines)\n",
    "\n",
    "        # For buildings\n",
    "        classes_to_be_removed = ['wall', 'awning', 'canopy', 'bridge', 'tree', 'palm tree', 'plant', 'grass']\n",
    "        inpainted_semantic_2 = inpaint_semantic(classes_to_black(inpainted_semantic, color_map, classes_to_be_removed), vanishing_lines, color_map)\n",
    "\n",
    "        # Create masks\n",
    "        structure_mask = create_mask(inpainted_semantic_2, structure_classes, color_map)\n",
    "        wall_mask = create_mask(inpainted_semantic, wall_classes, color_map)\n",
    "        vegetation_mask = create_mask(inpainted_semantic, vegetation_classes, color_map)\n",
    "\n",
    "        # Extract objects (those that aren't connected)\n",
    "        structure_objects = extract_objects(structure_mask, depth)\n",
    "        wall_objects = extract_objects(wall_mask, depth)\n",
    "        vegetation_objects = extract_objects(vegetation_mask, depth)\n",
    "\n",
    "        # Divide structure objects (vertical column method)\n",
    "        new_structure_objects = []\n",
    "        for obj in structure_objects:\n",
    "            split_masks = split_object_by_vanishing_lines(obj, vanishing_lines, img.shape)\n",
    "            new_structure_objects.extend(split_masks)\n",
    "\n",
    "        # Record metadata for all objects\n",
    "        valid_structure_objects = []\n",
    "        valid_vegetation_objects = []\n",
    "        valid_wall_objects = []\n",
    "        \n",
    "        # STRUCTURE\n",
    "        for i, obj in enumerate(new_structure_objects, 1):\n",
    "\n",
    "            # Get the depth\n",
    "            avg_depth_structure, object_mask = get_depth(obj, cleaned_semantic, depth, color_map, structure_classes)\n",
    "            if avg_depth_structure is None:\n",
    "                continue  # skip to the next object\n",
    "\n",
    "            # Get the height\n",
    "            pixel_height_structure = get_pixel_height(obj, vanishing_lines)\n",
    "            if pixel_height_structure is None:\n",
    "                continue  # skip to the next object\n",
    "            \n",
    "            # Record\n",
    "            valid_structure_objects.append(obj) \n",
    "            meta = get_object_metadata(coord_angle, obj, \"structure\", avg_depth_structure, pixel_height_structure)\n",
    "            if meta:\n",
    "                all_objects_info.append(meta)\n",
    "        \n",
    "        new_structure_objects = valid_structure_objects\n",
    "\n",
    "        # WALL\n",
    "        for obj in wall_objects:\n",
    "            \n",
    "            # Get the depth\n",
    "            avg_depth_wall, object_mask = get_depth(obj, cleaned_semantic, depth, color_map, wall_classes)\n",
    "            if avg_depth_wall is None:\n",
    "                continue  # skip to the next object\n",
    "\n",
    "            # Get the height\n",
    "            pixel_height_wall = get_pixel_height(obj, vanishing_lines)\n",
    "            if pixel_height_wall is None:\n",
    "                continue  # skip to the next object\n",
    "            \n",
    "            # Record\n",
    "            valid_wall_objects.append(obj) \n",
    "            meta = get_object_metadata(coord_angle, obj, \"wall\", avg_depth_wall, pixel_height_wall)\n",
    "            if meta:\n",
    "                all_objects_info.append(meta)\n",
    "\n",
    "        new_wall_objects = valid_wall_objects\n",
    "\n",
    "        # VEGETATION\n",
    "        for i, obj in enumerate(vegetation_objects, 1):\n",
    "\n",
    "            # Get the depth\n",
    "            avg_depth_vegetation, object_mask = get_depth(obj, cleaned_semantic, depth, color_map, vegetation_classes)\n",
    "            if avg_depth_vegetation is None:\n",
    "                continue  # skip to the next object\n",
    "\n",
    "            # Get the top and bottom y\n",
    "            top_y_vegetation, bottom_y_vegetation = get_top_bottom_y(obj)\n",
    "            trunk_y = min(300, (0.6 * avg_depth_vegetation + 196))\n",
    "            pixel_height_vegetation = trunk_y - top_y_vegetation\n",
    "            if (pixel_height_vegetation <= 0):\n",
    "                continue  # skip to the next object\n",
    "\n",
    "            # Record\n",
    "            valid_vegetation_objects.append(obj) \n",
    "            meta = get_object_metadata(coord_angle, obj, \"vegetation\", avg_depth_vegetation, pixel_height_vegetation)\n",
    "            if meta:\n",
    "                all_objects_info.append(meta)\n",
    "\n",
    "            '''\n",
    "            # Display for testing purposes\n",
    "            plt.figure(figsize=(3, 3)) \n",
    "            plt.imshow(obj, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Tree {i}: Depth =\", avg_depth_vegetation)\n",
    "            print(f\"Tree {i}: Top y = \", top_y_vegetation)\n",
    "            print(f\"Tree {i}: Trunk y = \", trunk_y)\n",
    "            '''\n",
    "            \n",
    "        new_vegetation_objects = valid_vegetation_objects\n",
    "\n",
    "        # Create object visualizations\n",
    "        structure_visualization, num_structures = object_visualization(structure_mask, new_structure_objects)\n",
    "        wall_visualization, num_walls = object_visualization(wall_mask, new_wall_objects)\n",
    "        vegetation_visualization, num_vegetations = object_visualization(vegetation_mask, new_vegetation_objects)\n",
    "\n",
    "        '''\n",
    "        # Display the triplets, the masks, and the objects\n",
    "        image_grid = [\n",
    "            (\"Original\", cv2.cvtColor(img, cv2.COLOR_BGR2RGB), None, 0, 0),\n",
    "            (\"Vanishing Lines\", cv2.cvtColor(vanishing_lines_img, cv2.COLOR_BGR2RGB), None, 0, 1),\n",
    "            (\"Depth Map\", depth, \"magma\", 0, 2),\n",
    "\n",
    "            (\"Segmentation Mask\", semantic_rgb, None, 1, 0),\n",
    "            (\"Cleaned\", cleaned_semantic, None, 1, 1),\n",
    "            (\"Inpainted\", inpainted_semantic, None, 1, 2),\n",
    "            (\"Inpainted2\", inpainted_semantic_2, None, 1, 3),\n",
    "        \n",
    "            (\"Structure Mask\", cv2.cvtColor(structure_mask, cv2.COLOR_BGR2RGB), None, 2, 0),\n",
    "            (\"Wall Mask\", cv2.cvtColor(wall_mask, cv2.COLOR_BGR2RGB), None, 2, 1),\n",
    "            (\"Vegetation Mask\", cv2.cvtColor(vegetation_mask, cv2.COLOR_BGR2RGB), None, 2, 2),\n",
    "            \n",
    "            (f\"Structures ({num_structures})\", cv2.cvtColor(structure_visualization, cv2.COLOR_BGR2RGB), None, 3, 0),\n",
    "            (f\"Walls ({num_walls})\", cv2.cvtColor(wall_visualization, cv2.COLOR_BGR2RGB), None, 3, 1),\n",
    "            (f\"Vegetation ({num_vegetations})\", cv2.cvtColor(vegetation_visualization, cv2.COLOR_BGR2RGB), None, 3, 2),\n",
    "        ]\n",
    "        display_images(image_grid, grid_shape=(4, 4), figsize=(16, 10))\n",
    "        '''\n",
    "\n",
    "        #display_objects(new_structure_objects, title_prefix=\"Structure\")\n",
    "        #display_objects(new_wall_objects, title_prefix=\"Wall\")\n",
    "        #display_objects(new_vegetation_objects, title_prefix=\"Vegetation\")\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_json_path, exist_ok=True)\n",
    "        \n",
    "        # Save JSON file\n",
    "        record_objects_to_json(folder_output_json, all_objects_info)\n",
    "        # print(f\"{coord_angle}.json saved\")\n",
    "        \n",
    "        # Reset for next folder\n",
    "        all_objects_info = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4555829",
   "metadata": {},
   "source": [
    "b. Combine JSON files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14611fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = output_json_path\n",
    "output_file = os.path.join(output_final_json_path, \"output.json\")\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "json_files = [f for f in os.listdir(input_folder) if f.endswith(\".json\")]\n",
    "\n",
    "for filename in tqdm(json_files, desc=\"Combining JSON files\"):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                combined_data.extend(data)\n",
    "            else:\n",
    "                combined_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping invalid JSON file: {filename}\")\n",
    "\n",
    "# Save combined output\n",
    "os.makedirs(output_final_json_path, exist_ok=True)\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(combined_data, f_out, indent=4)\n",
    "\n",
    "print(f\"\\nSaved combined JSON to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
